import sys
import os
import datetime
import pickle
import numpy as np
import matplotlib.pyplot as plt
from pprint import pprint
from scipy.optimize import curve_fit
from scipy import stats
from SP2_particle_record_UTC import ParticleRecord
from struct import *
import hk_new
import hk_new_no_ts_LEO
from scipy import linspace, polyval, polyfit, sqrt, stats
import math
import sqlite3



current_dir = 'D:/2012/WHI_UBCSP2/Calibrations/20120328/PSL/Binary/200nm/'
instrument = 'WHI_UBCSP2'
instrument_locn = 'WHI'
PSL_size = 200.
type_particle = 'PSL'
os.chdir(current_dir)

#setup
num_records_to_analyse = 10000
show_full_fit = False

#pararmeters used to reject invalid particle records based on scattering peak attributes
min_peakheight = 20
max_peakheight = 1750 #= no limit
min_peakpos = 20
max_peakpos = 125
scat_sat_amp = 1750


#**********parameters dictionary**********

parameters = {
'acq_rate': 5000000,
#file i/o
'directory':current_dir,
#date and time
'timezone':-8,
#will be set by hk analysis
'avg_flow':120, #in vccm
#parameter to find bad flow durations
'flow_min' : 115,
'flow_max' : 125,
'YAG_min' : 4,
'YAG_max' : 6,
'min_good_points' : 10,
#show plots?
'show_plot':False,
}

#setup database
conn = sqlite3.connect('C:/projects/dbs/SP2_data.db')
c = conn.cursor()

c.execute('''CREATE TABLE if not exists SP2_coating_analysis(
id INTEGER PRIMARY KEY AUTOINCREMENT,
sp2b_file TEXT, 
file_index INT, 
instr TEXT,
instr_locn TEXT,
particle_type TEXT,		
particle_dia FLOAT,				
date TIMESTAMP,
actual_scat_amp FLOAT,
actual_peak_pos INT,
FF_scat_amp FLOAT,
FF_peak_pos INT,
FF_gauss_width FLOAT,
zeroX_to_peak FLOAT,
LF_scat_amp FLOAT,
incand_amp FLOAT,
UNIQUE (sp2b_file, file_index, instr)
)''')

#particle_type = PSL, nonincand, incand
#particle_dia = only known for PSL  

#*******HK ANALYSIS************ 

###use for hk files with no timestamp (just time since midnight) (this should work for the EC polar flights in spring 2012,also for ECSP2 for WHI 20100610 to 20100026, UBCSP2 prior to 20120405)
avg_flow = hk_new_no_ts_LEO.find_bad_hk_durations_no_ts(parameters) 
parameters['avg_flow'] = avg_flow
#bad_durations = []

###use for hk files with timestamp (this is for the UBCSP2 after 20120405)
#avg_flow = hk_new.find_bad_hk_durations(parameters)
#parameters['avg_flow'] = avg_flow

#*************LEO routine************


for file in os.listdir('.'):
	
	if file.endswith('.sp2b'):
		
		print file
		
		path = current_dir + str(file)
		file_bytes = os.path.getsize(path) #size of entire file in bytes
		record_size = 1498 #size of a single particle record in bytes(UBC_SP2 = 1498, EC_SP2 in 2009 and 2010 = 2458)
		number_of_records = (file_bytes/record_size)-1
		if num_records_to_analyse == 'all':
			number_records_toshow =  number_of_records 
		else:
			number_records_toshow = num_records_to_analyse    
		
		##************This is the full-gauss prefit for PSLs************
		
		f = open(file, 'rb')
		
		#grab the pickled bad_durations file generated by the HK analysis
		for hk_file in os.listdir('.'):
			if hk_file.endswith('.hkpckl'):
				hk_data = open(hk_file, 'r')
				bad_durations = pickle.load(hk_data)
				hk_data.close()
	
		record_index = 0      
		
		while record_index < number_records_toshow:
			
			##Import and parse binary
			record = f.read(record_size)
			particle_record = ParticleRecord(record, parameters['acq_rate'], parameters['timezone'])	
			event_time = particle_record.timestamp
			
			###### FITTING AND ANALYSIS ########          
			number_bad_durations = len(bad_durations)
			
							
			#if there are any bad hk durations, note the beginning and end times of the first one
			if number_bad_durations:               
				bad_duration_start_time = bad_durations[0][0]
				bad_duration_end_time = bad_durations[0][1]
			
				#if the current event is after the end of the first bad duration in the list, pop that duration off, repeat if necessary until all bad durations before the event are gone
				while event_time >= bad_duration_end_time:
					if len(bad_durations): 
						bad_durations.pop(0)
						if len(bad_durations):
							bad_duration_start_time = bad_durations[0][0]
							bad_duration_end_time = bad_durations[0][1]
							continue
						else:
							break
			

			if not number_bad_durations or event_time < bad_duration_start_time:  

				#run the scatteringPeakInfo method to retrieve various peak attributes 
				particle_record.scatteringPeakInfo()		
				
				#check for a double peak
				particle_record.isSingleParticle()

				#note: zero-crossing calc will depend on the slope of the zero-crossing from the split detector
				zero_crossing_pt = particle_record.zeroCrossing()
				
				#check to see if scattering signal is over threshold, is in a reasonable position, and no double peaks
				if particle_record.scatteringMax > min_peakheight and particle_record.scatteringMax < max_peakheight and particle_record.scatteringMaxPos > min_peakpos and particle_record.scatteringMaxPos < max_peakpos and particle_record.doublePeak==False and zero_crossing_pt > 0 : 
										
					#set parameters for fitting
					baseline = particle_record.scatteringBaseline
					x_vals = particle_record.getAcqPoints()
					y_vals = particle_record.getScatteringSignal()
										
					#initial values for amplitude(a) center(u) and gauss width(sig)
					guess_a = particle_record.scatteringMax  
					guess_u = particle_record.scatteringMaxPos
					guess_sig = 17
					p_guess = [guess_a,guess_u,guess_sig]
					
					def fullGauss(x, a, u, sig):
						return baseline+a*np.exp((-(x-u)**2)/(2*sig**2))
					
					#run the fitting
					try:
						popt, pcov = curve_fit(fullGauss, x_vals, y_vals, p0=p_guess)
					except:
						popt, pcov = None, None   
						print 'full fail'
					
					if popt[0] != None:           
						#parameters to save
						scattering_amp = popt[0]
						fit_peak_pos = popt[1]          
						gauss_width = popt[2]
						actual_max_value = particle_record.scatteringMax
						actual_max_pos = particle_record.scatteringMaxPos
						
						#a neg zero-crossing value means an exception was thrown when zero crossing calculated (see particle_record class methods)
						if zero_crossing_pt >=0:
							zero_cross_to_peak = (zero_crossing_pt - fit_peak_pos)
						else:  
							zero_cross_to_peak = np.nan
						
						#put particle into database or update record
						c.execute('''INSERT or IGNORE into SP2_coating_analysis (sp2b_file, file_index, instr, instr_locn, particle_type, particle_dia) VALUES (?,?,?,?,?,?)''', (file, record_index,instrument, instrument_locn,type_particle,PSL_size))
						c.execute('''UPDATE SP2_coating_analysis SET 
						date=?, 
						actual_scat_amp=?, 
						actual_peak_pos=?, 
						FF_scat_amp=?, 
						FF_peak_pos=?, 
						FF_gauss_width=?, 
						zeroX_to_peak=?
						WHERE sp2b_file=? and file_index=? and instr=?''', 
						(event_time,
						actual_max_value,
						actual_max_pos,
						scattering_amp,
						fit_peak_pos,
						gauss_width,
						zero_cross_to_peak,
						file, record_index,instrument))
										
						#plot particle fit if desired
						fit_result = []
						if show_full_fit == True:
							for x in range(0,180):
								fit_result.append(fullGauss(x,popt[0],popt[1],popt[2]))
								
							print record_index, particle_record.scatteringMax
							
							fig = plt.figure()
							ax1 = fig.add_subplot(111)
							ax1.plot(x_vals,y_vals,'o', markerfacecolor='None')   
							ax1.plot(x_vals,fit_result, 'red')
							plt.show()

						

			record_index+=1   
				
		f.close()
		
conn.commit()
conn.close()


	
